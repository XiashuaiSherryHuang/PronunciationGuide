{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2Y6_bz9hcmiC",
        "outputId": "1e2ff223-9b69-4b19-866e-ce3d4315027b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (2.0.1+cu118)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch) (3.12.0)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from torch) (4.5.0)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch) (1.11.1)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch) (3.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch) (3.1.2)\n",
            "Requirement already satisfied: triton==2.0.0 in /usr/local/lib/python3.10/dist-packages (from torch) (2.0.0)\n",
            "Requirement already satisfied: cmake in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch) (3.25.2)\n",
            "Requirement already satisfied: lit in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch) (16.0.5)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch) (2.1.2)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch) (1.3.0)\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (4.29.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.12.0)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.14.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.15.1)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (1.22.4)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (23.1)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2022.10.31)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.27.1)\n",
            "Requirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.13.3)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.65.0)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.14.1->transformers) (2023.4.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.14.1->transformers) (4.5.0)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (1.26.15)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2022.12.7)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.0.12)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.4)\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: librosa in /usr/local/lib/python3.10/dist-packages (0.10.0.post2)\n",
            "Requirement already satisfied: audioread>=2.1.9 in /usr/local/lib/python3.10/dist-packages (from librosa) (3.0.0)\n",
            "Requirement already satisfied: numpy!=1.22.0,!=1.22.1,!=1.22.2,>=1.20.3 in /usr/local/lib/python3.10/dist-packages (from librosa) (1.22.4)\n",
            "Requirement already satisfied: scipy>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from librosa) (1.10.1)\n",
            "Requirement already satisfied: scikit-learn>=0.20.0 in /usr/local/lib/python3.10/dist-packages (from librosa) (1.2.2)\n",
            "Requirement already satisfied: joblib>=0.14 in /usr/local/lib/python3.10/dist-packages (from librosa) (1.2.0)\n",
            "Requirement already satisfied: decorator>=4.3.0 in /usr/local/lib/python3.10/dist-packages (from librosa) (4.4.2)\n",
            "Requirement already satisfied: numba>=0.51.0 in /usr/local/lib/python3.10/dist-packages (from librosa) (0.56.4)\n",
            "Requirement already satisfied: soundfile>=0.12.1 in /usr/local/lib/python3.10/dist-packages (from librosa) (0.12.1)\n",
            "Requirement already satisfied: pooch<1.7,>=1.0 in /usr/local/lib/python3.10/dist-packages (from librosa) (1.6.0)\n",
            "Requirement already satisfied: soxr>=0.3.2 in /usr/local/lib/python3.10/dist-packages (from librosa) (0.3.5)\n",
            "Requirement already satisfied: typing-extensions>=4.1.1 in /usr/local/lib/python3.10/dist-packages (from librosa) (4.5.0)\n",
            "Requirement already satisfied: lazy-loader>=0.1 in /usr/local/lib/python3.10/dist-packages (from librosa) (0.2)\n",
            "Requirement already satisfied: msgpack>=1.0 in /usr/local/lib/python3.10/dist-packages (from librosa) (1.0.5)\n",
            "Requirement already satisfied: llvmlite<0.40,>=0.39.0dev0 in /usr/local/lib/python3.10/dist-packages (from numba>=0.51.0->librosa) (0.39.1)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from numba>=0.51.0->librosa) (67.7.2)\n",
            "Requirement already satisfied: appdirs>=1.3.0 in /usr/local/lib/python3.10/dist-packages (from pooch<1.7,>=1.0->librosa) (1.4.4)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from pooch<1.7,>=1.0->librosa) (23.1)\n",
            "Requirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.10/dist-packages (from pooch<1.7,>=1.0->librosa) (2.27.1)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn>=0.20.0->librosa) (3.1.0)\n",
            "Requirement already satisfied: cffi>=1.0 in /usr/local/lib/python3.10/dist-packages (from soundfile>=0.12.1->librosa) (1.15.1)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.10/dist-packages (from cffi>=1.0->soundfile>=0.12.1->librosa) (2.21)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->pooch<1.7,>=1.0->librosa) (1.26.15)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->pooch<1.7,>=1.0->librosa) (2022.12.7)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->pooch<1.7,>=1.0->librosa) (2.0.12)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->pooch<1.7,>=1.0->librosa) (3.4)\n"
          ]
        }
      ],
      "source": [
        "!pip install torch\n",
        "!pip install transformers\n",
        "#https://huggingface.co/facebook/wav2vec2-large-960h-lv60-self\n",
        "#https://github.com/facebookresearch/fairseq/tree/main/examples/wav2vec#wav2vec-20\n",
        "#https://github.com/facebookresearch/fairseq/blob/main/examples/wav2vec/README.md#use-wav2vec-20-with-transformers\n",
        "!pip install librosa"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import librosa\n",
        "## Grapheme To Phoneme Conversion\n",
        "import torch\n",
        "#Importing Wav2Vec\n",
        "from transformers import Wav2Vec2ForCTC, Wav2Vec2Tokenizer, Wav2Vec2Processor\n",
        "import os"
      ],
      "metadata": {
        "id": "k4D_eOuNcsEt"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_arpabet_from_audio(file):\n",
        "    # Importing Wav2Vec pretrained model\n",
        "    repo_name = \"sherryh0112/wav2vec2-base-test2\"\n",
        "\n",
        "    processor = Wav2Vec2Processor.from_pretrained(repo_name) \n",
        "    model = Wav2Vec2ForCTC.from_pretrained(repo_name)\n",
        "\n",
        "    sr = 16000 #16000\n",
        "    audio, rate = librosa.load(file, sr = sr)\n",
        "    # Taking an input value\n",
        "    input_values = processor(audio, sampling_rate=rate, return_tensors = \"pt\").input_values\n",
        "\n",
        "    # INFERENCE\n",
        "    # Storing logits (non-normalized prediction values)\n",
        "    logits = model(input_values).logits\n",
        "    pred_p = processor.batch_decode(torch.argmax(logits, dim=-1), clean_up_tokenization_spaces=True)[0]\n",
        "    print('Predicted text from audio is {}'.format(pred_p))\n",
        "\n",
        "    return pred_p"
      ],
      "metadata": {
        "id": "Xnc4vz9AcyVu"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def write_text_to_file(filename, text):\n",
        "    try:\n",
        "        with open(filename, 'w') as file:\n",
        "            file.write(text)\n",
        "    except Exception:\n",
        "        print(f'An error occurred while writing to {filename}.')"
      ],
      "metadata": {
        "id": "A--zkRUldKDx"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "wav_directory = 'wav/'\n",
        "output_directory = 'wav2arpaf/'"
      ],
      "metadata": {
        "id": "pP2Y33XRdPZn"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "error_wav_files=['quechua2.wav', 'arabic42.wav']"
      ],
      "metadata": {
        "id": "K_dMGcZMeOBi"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "existing_files = os.listdir(output_directory)\n",
        "existing_files"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0TB0ZY7ieKpZ",
        "outputId": "d9a7551e-dc6b-4d69-8e6e-120d37c013be"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['mankanya1.txt',\n",
              " 'spanish3.txt',\n",
              " 'danish2.txt',\n",
              " 'arabic17.txt',\n",
              " 'greek2.txt',\n",
              " 'italian22.txt',\n",
              " 'hebrew3.txt',\n",
              " 'english38.txt',\n",
              " 'spanish19.txt',\n",
              " 'portuguese25.txt',\n",
              " 'luba-kasai1.txt',\n",
              " 'georgian5.txt',\n",
              " 'kirghiz1.txt',\n",
              " 'portuguese3.txt',\n",
              " 'cantonese1.txt',\n",
              " 'german21.txt',\n",
              " 'romanian4.txt',\n",
              " 'thai2.txt',\n",
              " 'mongolian3.txt',\n",
              " 'croatian6.txt',\n",
              " 'hausa7.txt',\n",
              " 'swedish2.txt',\n",
              " 'turkish9.txt',\n",
              " 'spanish17.txt',\n",
              " 'norwegian5.txt',\n",
              " 'japanese12.txt',\n",
              " 'arabic1.txt',\n",
              " 'nepali9.txt',\n",
              " 'farsi5.txt',\n",
              " 'farsi10.txt',\n",
              " 'english2.txt',\n",
              " 'vietnamese9.txt',\n",
              " 'uyghur4.txt',\n",
              " 'portuguese10.txt',\n",
              " 'bengali2.txt',\n",
              " 'slovak4.txt',\n",
              " 'hungarian5.txt',\n",
              " 'konkani2.txt',\n",
              " 'rwanda1.txt',\n",
              " 'serbian17.txt',\n",
              " 'english55.txt',\n",
              " 'arabic8.txt',\n",
              " 'shilluk1.txt',\n",
              " 'tokpisin1.txt',\n",
              " 'finnish1.txt',\n",
              " 'turkish15.txt',\n",
              " 'yoruba3.txt',\n",
              " 'krio2.txt',\n",
              " 'spanish47.txt',\n",
              " 'spanish4.txt',\n",
              " 'tagalog3.txt',\n",
              " 'yakut1.txt',\n",
              " 'arabic2.txt',\n",
              " 'russian25.txt']"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "os.listdir(wav_directory)[:10]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SbwK-4m6fv3R",
        "outputId": "75a832a6-f2e6-41cc-a07e-3fae08414e1b"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['portuguese25.wav',\n",
              " 'vietnamese9.wav',\n",
              " 'arabic8.wav',\n",
              " 'luba-kasai1.wav',\n",
              " 'yoruba3.wav',\n",
              " 'slovak4.wav',\n",
              " 'cantonese1.wav',\n",
              " 'spanish17.wav',\n",
              " 'arabic17.wav',\n",
              " 'croatian6.wav']"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for file_name in os.listdir(wav_directory):\n",
        "    if file_name.endswith('.wav'):\n",
        "      try:\n",
        "        print(file_name)\n",
        "        if file_name in error_wav_files:\n",
        "            continue\n",
        "        output_file = file_name.split('.')[0]+'.txt'\n",
        "        if output_file in existing_files:\n",
        "            continue\n",
        "        wav_file_path = os.path.join(wav_directory, file_name)\n",
        "        pred_p = get_arpabet_from_audio(wav_file_path)\n",
        "        output_path = os.path.join(output_directory, output_file)\n",
        "        print(output_path)\n",
        "        write_text_to_file(output_path, pred_p)\n",
        "      except Exception as e:\n",
        "        print(f'An error occurred while processing {file_name} {str(e)}.')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jwtXK3HedZ8i",
        "outputId": "e25b967a-76e3-47f2-ce93-5f802cd98d37"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "portuguese25.wav\n",
            "vietnamese9.wav\n",
            "arabic8.wav\n",
            "luba-kasai1.wav\n",
            "yoruba3.wav\n",
            "slovak4.wav\n",
            "cantonese1.wav\n",
            "spanish17.wav\n",
            "arabic17.wav\n",
            "croatian6.wav\n",
            "krio2.wav\n",
            "farsi10.wav\n",
            "norwegian5.wav\n",
            "thai2.wav\n",
            "japanese12.wav\n",
            "danish2.wav\n",
            "english2.wav\n",
            "spanish19.wav\n",
            "hausa7.wav\n",
            "portuguese3.wav\n",
            "romanian4.wav\n",
            "mankanya1.wav\n",
            "nepali9.wav\n",
            "uyghur4.wav\n",
            "english55.wav\n",
            "quechua2.wav\n",
            "hungarian5.wav\n",
            "russian25.wav\n",
            "farsi5.wav\n",
            "tagalog3.wav\n",
            "serbian17.wav\n",
            "konkani2.wav\n",
            "spanish47.wav\n",
            "georgian5.wav\n",
            "italian22.wav\n",
            "rwanda1.wav\n",
            "mongolian3.wav\n",
            "swedish2.wav\n",
            "finnish1.wav\n",
            "shilluk1.wav\n",
            "bengali2.wav\n",
            "arabic2.wav\n",
            "turkish9.wav\n",
            "hebrew3.wav\n",
            "portuguese10.wav\n",
            "spanish3.wav\n",
            "yakut1.wav\n",
            "greek2.wav\n",
            "turkish15.wav\n",
            "english38.wav\n",
            "arabic1.wav\n",
            "german21.wav\n",
            "tokpisin1.wav\n",
            "spanish4.wav\n",
            "kirghiz1.wav\n",
            "arabic42.wav\n",
            "french6.wav\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Predicted text from audio is PLIY1Z KAO1L STEH0LAE0 AE1SK ER1 TUW1 BRIH1NG DIY1S FHIH1NGS WIH1F EH1 FRAH1M DAX1 STAO1 SIH1KS SPUW1NS AO1F FREH1TH SNOW1H PIY1S FAE0IH0F THIY1K SLAE1BS AO1F BLUW1 CHIY1S AE1N MEH0IH0BIY0 AX1 SNAE1K FAO1 HHE1 BRAH0DHAX0 BAO1B WIY1 AO0LSOW0 NIY1D AX1 SMAO1L PLAE0STIY0K SNEH0IH0K AEH1N AX1 BIY1G TOY1 FRAO1G FAO1R DAX1 KIY1DS SHIY1 KE1N SKUW1P DIY1S FIH1NGS IH0NTUW0 FRIY1 REH1D BAE1GS AE1N WIY1 WIH1L GOW01 MIY1T HHER1 WEH0NZDEH0IH0 AE1T DAX1 TREH0IH0N STEH0IH0SHAX0N\n",
            "wav2arpaf/french6.txt\n",
            "english44.wav\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Predicted text from audio is PLIY1Z KAA1L STEH0LAX0 AE1SK HHER1R DXAX1 BRIH1NG DHIY1 THIH1NGZ WIH1TH HHER1R FRAX1M DHAX1 STAO01XR SIH1KS SPUW1NZ AX1V FREH1SH SNOW0U1H0 PIY1Z FAE1V THIH1K SLAE1BZ AX1V BLUW1 CHXIY1Z AH1N MEH0IH0BIY0 AX1 SNAE1K FAX1 HHER1 BRAH0DHAX0 BAA1B WIY1 AA0LSOW0 NI0IY1D EH1 SMAO1L PLAE0STIH0K SNAEH0IH0K E1N EH0H0 BIH1G TOY1 FRAA1G FAX1R DHAX1 KI0IHX0DZ SHIY1 KAX1N SKUW1P DHIY1Z THIH1NGZ IH0NTUW0 THRIY1 REH1D BAE1GZ AH1N WIY1 WIH1L GOW0UH0 MIY1T HHER1 WIH0NZDEH0IH0 AE1T DHAX1 TREH0IH0N STEH0IH0SHAX0N\n",
            "wav2arpaf/english44.txt\n",
            "gujarati5.wav\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
            "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Predicted text from audio is PLIY1S KAO1L STEH0LAE0 AE1SK HER1R DTUW1 BRIH1NG DIY1ZS THIH1NGZ VIH1TH HHER1R FRAH1M DAX1 STAO1R SIH1KS SPUW1NZ AX1F FREH1SH SNOW0UH PIY1S FAE0IH0F THIH1K SLAE1BZ AX1V BLUW1 CHIY1Z AE1N MEH0IH0BIY0 AX1 SNAE1K FAO1R HHER1R BRAH0DHAX0 BAOW1B VIY1 AO0LSOW0 NIY1D AX1 SMAO1L PLAE0SIH0K SNEH0IH0K AE1N AX1 BIH1K TOY1 FRAO1G FAO1 DHAX1 KIH1DS SHIY1 KAX1N SKUW1P DIY1S THIH1NGZ IH0NTUW0 TRIY1 REH1D BAE1GS AE1N VIY1 VIH1L GOW1H MIY1T HHER1 VEH0NAX0ZDEH0IH0 AE1T DHAX1 TREH0IH0N STEH0IH0SHAX0N\n",
            "wav2arpaf/gujarati5.txt\n",
            "english139.wav\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Predicted text from audio is PLIY1Z KAA1L STEH0LAX0 AE1SK HER1 DXAX1 BRIH1NG NIY1Z SIH1NGZ WIH1TH HHER1 FRAH1M NAX1 STAO1R SIH1KS SPUW1NZ AX1V FREH1SH SHNOW0UH0 PIY1Z FAE0IH0V THIH1K SLAE1BZ AX1V BLUW1 CHIY1Z AX1N MEH0IH0BIY0 AX1 SNAE1K FAX1 HHER1 BRAH0DHAX0 BAA1B WIY1 AO0LSO0 NIY1DX AX1 SMAA1L PLAE0STIH0K SNEH0IH0K AX1N AX1 BIH1G TOY1 FRAA1G FAX1 DHAX1 KIH1DZ SHIY1 KAX1N SKUW1P DIY1Z SIH1NGZ IH0NDXAX0 THRIY1 REH1D BAE1GZ AX1N WH1L GOW0UH0 MIY1T HHER1 WEH0ZDEH0IH0 AE1T DHAX1 TREH0IH0N STEH0IH0SHAX0N\n",
            "wav2arpaf/english139.txt\n",
            "dutch41.wav\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Predicted text from audio is PLIY1S KAO1L STEH0LAE0 AE1SK HHER1 TUW1 BRIH1NG DIY1T THIH1NGS WIH1S HHER1 FRAH1M DAX1 STAO1R SIH1KS SPUW1NS AO1F FREH1SH SNOW0UH PIY1S FAE0IH0F THIH1K SLAE1BS AX1V BLUW1 CHIY1S AE1N MEH0IH0BIY0 AX1 SNAE1K FAO1 HEAR1 BRAH0DAX0 BA1B WIY1 AO0LSOW0 NIY1D AX1 SMAO1L PLAE0STIH0K SNEH0IH0K AE1N AX1 BIH1G TOY1 FRAO1G FAO1R DAX1 KIH1TS SHIY1 KAE1N SKUW1P DIY1 THIH1NGS IH0NTUW0 THRIY1 REH1D BAE1KS AX1ND WIY1 WIH1L GOW0UH MIY1T HHER1 WEH0NSDEH0IH0 AE1T DAX1 TREH0IH0N STEH0IH0SHAX0N\n",
            "wav2arpaf/dutch41.txt\n",
            "portuguese4.wav\n",
            "Predicted text from audio is PLIY1S KAO1L STEH0LAX0 AE1S HHER1 TUW1 BRIH1NG DIH1S FIH1NGS WIH1F HHE1 FRAH1M DAX1 STAO1R SIH1KS SPUW1NZ AO1F FREH1S SNOW1 PIY1S FAE0IH0F TIY1K SLAE1BZ AO1F BLUW1 SHIY1S E1N MEH0IH0BIY0 EH1 SNE1K FAO1R HHER1 BRAH0DHAX0 BOW1 WIY1 AO0LSOW0 NIY1D EH1 SMAO1L PLAE0SIH0K SNEH0IH0K AE1N EH1 BIY1G TOY1 FRAO1G FRAO1M DAX1 KIY1TS SHIY1 KAE1N SKUW1P SHIY1 KE1N SKUW1P DIY1S FIH1NGS IH0NTUW0 TRIY1 REH1D BAE1GS AEH1N WIY1 WIH1L GOW1 MIY1T HHER1R WEH0NZDEH0IH0 AE1T DAX1 TREH0IH0N STEH0IH0SHAX0N\n",
            "wav2arpaf/portuguese4.txt\n",
            "dari6.wav\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Predicted text from audio is PLIY1S KAO1L STEH0LAX0 AE1SK HHER1R TUW1 BRIH1NG DIY1Z TIH1NGS WIH1T HHER1 FRAO1M DAX1 STAO1R SIH1KS AX0SPUW0NS AO1F FREH1SH AX0SNOW1H PIY1S FAE0IH0F TIH1KS SLAE1BS AX1F BLUW1 CHIY1S AE1N MEH0IH0BIY0 EH1 SNAE1K FAO1R HHEH1R BRAH0DHEX0 BAO1P VIY1 AO0LSOW0 NIY1D EH1 SMAO1L PLAE0STIH0K SNEH0IH0K AE1N AX1 BIH1G TOY1 FRAO1K FAO1R DAX1 KIH1TS SHIY1 KAE1N AX0SKUW1P DIY1S TIH1NG IH0NTUW0 TRIY1 REH1D BAE1KS AE1ND AX1 IY1 WIH1L GOW1 MIY1T HHER1R WEH0NZDEH0IH0 AE1T DAX1 TREHIH0N STEH0IH0SHAX0N\n",
            "wav2arpaf/dari6.txt\n",
            "ukranian8.wav\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
            "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Predicted text from audio is PLIY1S KAO1L STEH0LAX0 AE1S HHER1R TUW1 BRIH1NG DIH1S AEH1 THIH1NGZ WIH1Z HHER1R FRAH1M NAX1 STAO1R SIH1KS SPUW1NZ AX1F FREH1SH SHNOW0UH0 PIY1Z FAE1V THIH1K SLAE1BZ AX1V BLUW1 CHIY1Z AE1N MEH0IH0BIY0 AX1 SNAE1K AO1F FAO1R HHER1 BRAH0DHAX0 BAA1B WIY1 AO0LSOW0 NIY1D AX1 SMAO1L PLAE0STIH0KS AE1 SNEH0IH0K EH1N AX1 BIY1G TAOY1K AX1 TOY1 FRAA1G FAO1 DAX1 KIH1D AE1 SHIY1 KAX1N SKUW1P DIH1S SIH1NGZ IH0NTUWX0 TRIY1 REH1D BAE1GS AE1N WIY1 WIH1L GOW01H0 MIY1T HHER1R WEH0NZDEH0IH0 AE1T DAX1 TREH0IH0N STEH0IH0SHAX0N\n",
            "wav2arpaf/ukranian8.txt\n",
            "tamil3.wav\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Predicted text from audio is PLIY1Z KAOW1L STEH0LAX0 AE1SK EX1 TAX1 BRIH1NG DHIY1Z THIH1NGZ WIH1TH HHER1 FRAH1M DHAX1 STAO1 SIH1KS SPUW1NZ AX1 FREH1SH SHNOW01 PIY1Z FAE0IH0V THIH1K SLAE1BZ AX1V BLUW1 CHIY1Z AE1N MEH0IH0BIY0 AX1 SNAE1K FAO1 HEX1 BRAH0DHAX0 BOW1B VIY1 AO0LSOW0 NIY1D AX1 SMAO1L PLAE0SIH0K SNEH0IH0K AE1ND AX1 BIH1G TOY1 FRAO1G FAO1 DHAX1 KIH1DS SHIY1 KE1N SKUW1P DHIY1Z THIH1NGZ IH0NTAX0 THRIY1 REH1D BAE1GZ AE1N VIY1 WIH1L GOW01H MIY1T HHER1 VEH0NAX0ZDEH0IH0 AE1T DHAX1 TRH0IH0N STEH0IH0SHAX0N\n",
            "wav2arpaf/tamil3.txt\n",
            "somali3.wav\n",
            "Predicted text from audio is PLIY1S KAO1L IH0SSTIY0LAE0 AE1SKAH HHAE1R TUW1 BRIH1NG DIY1ZS THIH1NGS WIH1DA HHAE1R FRAO1M DIY1 STAO1R SIH1 AH0SA0SPUW10NS AO1F FREH1SH AX0SNOWUH PIY1S FAE0IH0F THIH1K IH0SSAX SLEH0PTIY0S AO1F BLUW1 CHIY1S AE1NX MEH0IH0BIY0 EH1  S0NEH0IH0K FAO1R HHAE1 BRAE0DHAX0R BAOW1P WIY1 AO0LSOW0 NIY1D EHH SMAO1L PLAE0STIH0K SNAE1K AE1N EH1 BIH1K TOY1 FRAO1M FAO1R DHIX1 KIH1AXS SHIY1 KAE1N SKUW1P DIY1Z THIH1NGIH0S IH0NTUW0 TRIY1AX REH1D BAE1KS AE1N WIY1 WIH1L GOW1 MIY1T  HHAE1R WEH0NDEH0IH0 AE1T DAX1 TREH0IH0N STEH0IH0SHAX0N\n",
            "wav2arpaf/somali3.txt\n",
            "english112.wav\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Predicted text from audio is PLIY1Z KAE1L STEH0LAX0 AE1V HHER1R TUW1 AE1B AE1SK ER1 TUW1 BRIH1NG DHIY1Z THIH1NGZ WIH1TH HHER1 FRAH1M DHAX1 STAO1R SIH1KS SPUW1NZ AOW1Z FREH1SH SNOW0UH0 PAXIY1Z FAE0IH0V THIH1K SLAE1BZ AX1V BLUW1 CHIY1Z AE1N MEH0IH0BIY0 EH1 SNAE1K FAO1R HHER1R BRAH0DHAX0 BAA1B WIY1 AA0LSOW0 NIY1DX EH0H1 SMAA1L PLAE0STIH0K SNEH0IH0K AE1N EH0NTUW BIH1G TOY1 FRAA1G FAO1R DHAX1 KIH1DZ SHIY1 KAE1N SKUW1P DHIY1Z THIH1NGZ IH0NTUW0 THRIY1 REH1D BAE1GZ AE1N IY1 WIH1L GOW1 MIY1T HHER1 WEH0NZDEH0IH0 AE1T DHAX1 TREH0IH0N STEH0IH0SHAX0N\n",
            "wav2arpaf/english112.txt\n",
            "korean20.wav\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
            "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Predicted text from audio is PLIY1Z KAA1L STEH0LAE0 AE1SK HHER1 TUW1 BRIH1NG DIY1Z THIH1NGZ WIH1TH HHER1R FRAH1M DAX1 STAO1R SIH1KS SPUW1NZ AX1V FREH1SH SNOW0UH PIY1Z FAE0IH0F THIH1K S0LAE1BZ AX1V BLUW1 CHIY1Z AE1N MEH0IH0BIY0 AX1 SNAE1K FAO1 HHER1 BRAH0DHAX0 BA1B WIY1 AO0LSOW0 NIY1D AX1 SMAO1L PLAE0STIH0K SNEH0IH0K AE1N AX1 BIH1G TOY1 FRAO1G FAO1R DAX1 KIH1DZS SHIY1 KAH1N SKUW1P DIY1Z SHIH1NGZ IH0NTUW0 THRIY1 REH1D BAE1GZ AE1N WIY1 WIH1L GOW01H MIY1T HHER1 WEH0NZDEH0IH0 AE1T DAX1 TREH0IH0N STEH0IH0SHAX0N\n",
            "wav2arpaf/korean20.txt\n",
            "german15.wav\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Predicted text from audio is PLIY1S KAO1L STEH0LA0 AE1SK HHER1 TUW1 BRIH1NG DIY1Z THIH1NGS WIH1TH HHER1 FRAH1M DAX1 STAO1R SIH1KS SPUW1NS AX1F FREH1SH SNOW0UH0 PIY1S FAE0IH0V THIH1K SLAE1BS AX1V BLUW1 CHIY1Z AE1N MEH0IH0BIY0 AX1 SNAE1K FAO1 HHER1R BRAH0DHAX0 BAA1B WIY1 AA0LSO0 NIY1D AX1 SMAO1L PLAE0STIH0K SNEH0IH0K E1ND AX1 BIH1K TOY1 FRAA1G FAO1R DAX1 KIH1DS SHIY1 KAX1N SKUW1P DHIY1S THIH1NGS IH0NTUW0 THRIY1 REH1D BAE1KS AE1ND WIY1 WIH1L GOW0UH0 MIY1T HHER1 WEH0NSDEH0IH0 AE1T DHAX1 TREH0IH0N STEH0IH0SHAX0N\n",
            "wav2arpaf/german15.txt\n",
            "mandarin24.wav\n",
            "Predicted text from audio is PLIY1S KO1L STEH0LAE0 AE1SK HHER1 TUW1 BRIH1NG DHIY1S THIH1NGZ WIH1TH HHER1 FRAH1M DAX1 STAO1 SIH1KS SPUW1N AO1F FREH1SH SNOW0UH0 PIY1Z FAE0IH0F THIH1K SLAE1PS AX1F BLUW1 CHIY1S E1N MEH0IH0BIY0 AX1 SNAE1K FAO1 HHER1 BRAH0DHAX0 BAA1B WIY1 AO0LSOW0 NIY1D AX1 SMAO1L PLAE0STIH0K SNEH0IH0K AE1N AX1 BIH1K TOY1 FRAO1G FAO1 DHAX1 KIH1DS SHIY1 KAEH1N SKUW1P DHIY1S THIH1NGZ IH0NTUW0 THRIY1 RAEH1D BAE1GS AEH1N WIY1 WIH1L GOW0U0 MIY1T HHER1 WEH0NZDEH0IH0 AE1T DAX1 TREH0IH0N STEH0IH0SHAX0N\n",
            "wav2arpaf/mandarin24.txt\n",
            "macedonian1.wav\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
            "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Predicted text from audio is PLIY1S KAO1L STEH0LAX0 AE1SK HHER1R TUW1 BRIH1NG DIY1S TIH1NGKS WIH1TH HHER1R FRAH1M DAX1 STAO1R SIH1KS SPUW1NS AO1F FREH1SH ASNOW1H PIY1S FAE0IH0F TIH1K SLAE1PS AO1V BLUW1 CHIY1S AEH1N MEH0IH0BIY0 AX1 SNAE1K FAO1R HHER1 BRAH0DHER0 BA1B WIY1 AO0LSOW0 NIY1D AX1 SMAO1L PLAE0STIH0KSNEH0H0K EH1N AX1 BIY1K TOY1 FRAO1GK FAO1R DAX1 KIH1TS SHIY1 KEH1N SKUW1P DIY1S TIH1NGKS IH0NTUW0 TRIY1 REH1D BAE1KS EH1N WIY1 WIH1L GOW01H MIY1T HHER1R WEH0NZDEH0IH0 AE1T DAX1 TREH0IH0N STEH0IH0SHAX0N\n",
            "wav2arpaf/macedonian1.txt\n",
            "bosnian3.wav\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Predicted text from audio is PLIY1S KAO1L STEH0LAE0 AE1SK HHER1 TUW1 BRIH1N DIY1S TIH1NGS VIH1T HHER1R FRAH1M DAX1 STAO1R SIH1KS SPUW1NS AX1F FREH1SH SNOW1 PIY1S FAE0IH0 TIH1K SLAE1BS AX1F BLUW1 CHIY1Z AE1N MEH0IH0BIY0 AX1 SNAE1K FAO1R HHER1 BRAH0DHEX0 BAA1B VIY1 AO0LSOW0 NIY1D AX1 SMAO1L PLAE0STIH0K SNEH0IH0K AEH1ND AX1 BIY1K TOY1 FRAO1G FO1R DAX1 KIH1DS SHIY1 KAH1N SKUW1B DIY1S TIH1NG IH0NTUW0 TRIY1 REH1D BAE1KS AH1N VIY1 VIH1L GOW1 MIY1T HHER1 VEH0NZDEH0IH0 AE1T DAX1 TREH0IH0N STEH0IH0SHAX0N\n",
            "wav2arpaf/bosnian3.txt\n",
            "mandarin1.wav\n",
            "Predicted text from audio is PLIY1Z KAO1L STEH0LAE0 AE1SK HHER1R TUW1 BRIH1NG DHIY1 THIH1NGZ WIH1S HHER1R FRAH1M DHAX1 STAO1R SIH1KS SPUW1NS AX1V FREH1SH SNOW0UH PIY1S FAEI1V THIY1K SLAE1BS AX1V BLUW1 CHIY1S AE1ND MEH0IH0BIY0 AX1 SNEH1K FO1R HHER1 BRAH0DHAX0 BAO1B WIY1 AA0LSOW0 NIY1D AX1 SMAO1L PLAE0STIY0K SNEH1K AE1ND AX1 BIY1K TOY1 FRAO1G FRAH1M DHAX1 KIH0IHY1DZ SHIY1 KAE1N SKUW1P DHIY1 THIH1NGZ IH0NTUW0 THRIY1 REH1D BAE1KS AE1ND WIY1 WIH11 GOW0U MIY1T HHER1 WEH0NZDEH0IH0 AE1T DHAX1 TREH0IH1N STEH0IH0SHAX0N\n",
            "wav2arpaf/mandarin1.txt\n",
            "swedish4.wav\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
            "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Predicted text from audio is PLIY1S KAO1L STEH0LAE0 AE1SK HER1R TUW1 BRIH1NG DIY1S FIH1NGS WIH1T HER1 FRAH1M DAX1 STAO1R SIH1KS SPUW1NS AX1F FREH1SH SHNOW0UH PIY1S FAE0IH0F FIH1K SLAE1BZ AX1F BLUW1 SHIY1S AE1ND MEH0IH0BIY0 AX1 SNAE1K FAX1R HAER1 BRAH0DER0 BAO1B WIY1 AA0LSOW0 NIY1D AX1 SMAO1L PLAE0STIH0K SNEH0IH0K AH1N AX1 BIH1G TOY1 FRAO1G FAO1R DAX1 KIH1DS SHIY1 KAX1N SKUW1P DIY1Z FIH1NGKS IH0NTUW0 FRIY1 REH1D BAE1GS AH1N WIY1 WIH1L GOW0UH0 MIY1T HER1R WEH0NZDEH0IH0 AE1T DAX1 TREH0IH0N STEH0IH0SHAX0N\n",
            "wav2arpaf/swedish4.txt\n",
            "serbian10.wav\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Predicted text from audio is PLIY1Z KAA1L STEH0LA0 AE1S HHER1R TUW1 BRIH1NG DIH1S TIH1NGS WIH1T HHER1R FRAH1M DAX1 STAO1R SIH1KS SPUW1NZ AX1F FREH1SH SNOW0UH PIY1Z FAE0IH0F THIH1K SLAE1BZ A1V BLUW1 CHIY1Z EX1N MEH0IH0BIY0 AX1 SNAE1K FAO1R HHER1 BRAH0DHAR0 BAA1B WIY1 AO0LSOW0 NIY1D AX1 PLAE0STIH0K SNEH0IH0K AE1N AX1 BIH1G TOY1 FRAA1G FRAO1G FAO1R DAX1 KIH1DS SHIY1 KEH1N SKUW1B DIH1S TIH1NGZ IH0NTUW0 THRIY1 REH1D BAE1GS E1N WIY1 WIH1L GOW0UH MIY1T HHER1R WEH0NZDEH0IH0 AE1T DAX1 TREH0IH0N STEH0IH0SHAX0N\n",
            "wav2arpaf/serbian10.txt\n",
            "turkish20.wav\n",
            "Predicted text from audio is PLIY1S KAO1L STEH0LAE0 AE1SK HER1R DTUW1 BRIH1NG DIH1S TIH1NGS WIH1T HHER1 FRAH1M DAX1 STAO1 SIH1KS SPUW1Z AX1F FREH1S SNOW01H PIY1S FAE1F THIH1K SLAE1PS AX1F BLUW1 CH0IY1S EH1N MEH0IH0BIY0 AX1 SNAE1K FAO1 HHER1 BRAH0DAX0 BOW1B WIY1 A0LSOW0 NIY1DX AX1 SMAO1L PLAE0STIH0K SNAE1K E1N AX1 BIH1G TOY1 FRAO1G FAO1R DAX1 KIH1S SHIY1 KAE1N SKUW1P DIH1S THIH1NGS IH0NTUW0 TRIY1 AX1 RAE1B BAE1KS AE1N WIY1 VIH1L GOW01H MIY1T HHER1 VEH0NZDEH0IH0 AE1T DAX1 TREH0IH0N STEH0IH0SHAX0N\n",
            "wav2arpaf/turkish20.txt\n",
            "bulgarian1.wav\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
            "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Predicted text from audio is PLIY1S KAO1L STEH0LAE0 AE1S HHER1R TUW1 BRIH1NG DIY1S TIH1NGKS WIH1T HHER1R FRAH1M DAX1 STAO1R SIY1KS SPUW1NS AO1F FREH1SH SNOW01H PIY1Z FAE0IH0F THIY1K SLAE1BS AX1V BLUW1 CHIY1Z EH1N MEH0IH0BIY0 AX1 SNAE1K FAO1R HHE1R BRAH0DEX0 BAO1B WIY1 A0LSOW0 NIY1D AX1 SMAO1L PLAE0STIY0K SNEH0IH0K AE1N AX1 BIH1G TOY1 FRAO1G FAO1R DAX1 KIY1DS SHIY1 KEH1N SKUW1P DIY1S TIH1NGS IH0NTUW0 TRIY1 REH1D BAE1GZ EH1 WIY1 WIH1L GOW0UH MIY1T HHER1R WEH0NZDEH0IH0 AE1T DAX1 TREH0IH0N STEH0IH0SHAX0N\n",
            "wav2arpaf/bulgarian1.txt\n",
            "wolof3.wav\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Predicted text from audio is PLIY1Z KAO1L STEH0LAE0 AE1SK HHER1R TUW1 BRIH1NG DI1S FIH1NGS WIH1 HHER1 FRAH1M DAX1 STAO1R SIH1KS SPUW1NS AO1F FREH1SH SNOW1 PIY1S FAE0IH0F THIH1K SLAE1BS AO1F BLUW1 CHIY1S E1N MEH0IH0BIY0 AX1 SNAE1K AO1F HHER1R BRAH0DHAX0 BAOW1P IY1 AO0LSOW0 NIY1D AX1 SMAO1L PLAE0STIH0K SNEH0IH0K EH1N AX1 BIY1K TOY1 FRAO1G FAO1R DAX1 KIH1DS DIH1 KAE1N SKUW1P DIY1 THIH1NGS IH0NTUW0 THRIY1 REH1D BAE1GS EH1N WIY1 WIH1L GOW1 MIY1T HHER1 WEH0NSDEH0IH0 AE1T DAX1 TREH0IHN STEH0IH0SHAX0N\n",
            "wav2arpaf/wolof3.txt\n",
            "tibetan2.wav\n",
            "Predicted text from audio is PLIY1S KAO1L STIH0LAEX0 AE1SK EH1 HHER1 TUW1 BRIH1NG DHIY1S THIY1NGS WIH1S HHER1 FRAO1M DAH1 STAO1 SIY1KS SSPO1P SPUW1NS AO1F FREH1SH SNOW01H PIY0AHX0ZS FAE0IH0V THIY1K AX1 SLAE10PA0 AO1F BLUW1 CHIY1S AE1ND AX1 MEH0IH0BIY0 AH1 SNAE1K FAO1 HHER1R BRAE0DAX0 BOW1P WIY1 AO0LSOW0 NIY1D ER1 SNAE10 AH1 SPAE1N CHIY1K AEH1 SNEH0IH0K AE1ND AX1 BIY1K TOY1 FOW1 FAO1 DAX1 KIY1DS SHIY1 KAE1N SKUW1P DHIY1S THIH1NGKS IH0NTUW0 THRIY1 REH1D BAE1KS AE1NDAX0 WIY1 WIY1H1L GOW1 MIY1T HHER1 WEH1ND NIY1S DEH0IH0 AE1K THER1N NIY1NG STEH0IH0SHAX0N\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "wav2arpaf/tibetan2.txt\n",
            "hindi3.wav\n",
            "Predicted text from audio is PLIY1Z KAO1L STEH0LAX0 AE1SK HHER1R TUW1 BRIH1NG DIY1Z THIH1NGZ VIH1DH HHER1 FRAH1M DHAX1 STAO1R SIH1KS SPUW1NZ AX1F FREH1SH SNOW0UH PIY1Z FAE0IH0V THIH1K SLAE1BZ AX1V BLUW1 CHIY1Z AE1N MEH0IH0BIY0 AX1 SNAE1K FAO1R HHER1 BRAH0DHAX0 BAA1B VIY1 AA0LSOW0 NIY1D AX1 SMAO1L PLAE0STIH0K SNEH0IH0K AE1N AX1 BIH1G TOY1 FRAO1G FAO1R DHAX1 KIH1DZ SHIY1 KAH1N SKUW1P DIY1Z THIH1NGZ IH0NTUW0 THRIY1 REH1D BAE1GZ AE1N VIY1 VIH1L GOW0UH MIY1T HHER1R VEH0NZSDEH0IH0 AE1T DAX1REH1 AE1T DAX1 TREH0IH0N STEH0IH0SHAX0N\n",
            "wav2arpaf/hindi3.txt\n",
            "spanish58.wav\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Predicted text from audio is PLIY1S KAO1L STEH0LAE0 AE1SK HHER1R TUW1 BRIH1NG DHIY1Z THIH1NGS WIH1TH HHER1 FRAH1M DHAX1 STAO1R SIH1KS SPUW1NS AX1F FREH1SHSNOW0U1H PIY1S FAE0IH0V THIH1K SLAE1BS AX1V BLUW1 CHIY1S AE1N MEH0IH0BIY0 AEH1 SNAE1K FAO1R HHER1 BRAH0DHEX0 BAO1B WIY1 AO0LSOW0 NIY1D AX1 SMAO1L PLAE0STIH0K SNEH0IH0K AE1N AX1 BIH1G TOY1 FRAA1G FAO1R DHAX1 KIH1DTS SHIY1 KAEH1N SKUW1P DHIY1SZ THIH1NGS IH0NTUW0 THRIY1 REH1D BAE1GS EH1N WIY1 WIH1L GOW01UH0 MIY1T HHER1R WEH0NZDIH0 AE1T DAX1 TREH0IH0N STEH0IH0SHAX0N\n",
            "wav2arpaf/spanish58.txt\n",
            "french34.wav\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Predicted text from audio is PLIY1S KAO1L STEH0LAE0 AE1SK ER1 TUW1 BRIH1NG DIY1Z TIH1NG WIH1FH HHER1 FRAO1M DIH1 STAO1R SIY1KS SPUW1N AO1F FREH1SH SNOW1H PIY1Z FAE0IH0F TIH1K SLAE1BZ AO1F BLUW1 CHIY1Z EH1N MEH0IH0BIY0 EH1 SNAE1K FAO1R HHER1 BRAO0DXAX0 BOW1B WIY1P AO0LSOW0 NIY1D EH1 SMAO1L PLAE0STIH0K SNEH0IH0K AEH1N EH1 BIY1K TOY1 FRAO1G FAO1R DIAX1 KIY1TS SHIY1 KEH1N SKUW1P DIY1Z TIH1NGS IH0NTUW0 TRIY1 REH1D BAE1KS E1N WIY1 WIH1L GOW1 MIY1T HER1 WEH0NZDEH0IH0 ROW1 AE1T DAX1 TREH0IH0N STEH0IH0SHAX0N\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "wav2arpaf/french34.txt\n",
            "arabic24.wav\n",
            "Predicted text from audio is PLIY1S KAO1LA SIH1 AH1 SIH0TLAEH0 AE1SK HHEH1R TUW1 BRIH1NG EH1 DHIH1S SIH1NGS WIH1Z HHEH1R FRAO1M DAX1 STAO1R SIH1KS SPUW1NS A AOW1F FREH1SH H SNAOW1 PIY1S AEH1 FAE0IH0V DIH1S SIH1NGK EH0 THIH1K SLEH1P O1V BLUW1 SHIY1S AE1NDAEXH1 MEH0IH0BIY0 AE1 SNEH1K AO1F FAO1R HH1R AH1 BRAH0ZAX0R BAOW1B WIY1 AO0LSOW0 MIY1T AX1 SMAO1L PLAE0STIH0KEH0SNEH1K AE1ND 1 BIH1G AEH1 TOY1 FAO1RAX1 FRAO1G AH1 FAO1R ZAX1 KIY1DS AE1 SHIY1 KAE1N SKUW1P AEH1 DHIH1S SIH1NGS IH0NTUW0 THRIY1 REH1D BEH1GS AE1NDIH WIY1 WIH1L GOW1 MIY1T MIY1T HHEH1R WIH0NAX0ZDEH0IH0 AE1T DAX1 TREH0N STEHIH0SHAX0\n",
            "wav2arpaf/arabic24.txt\n",
            "russian20.wav\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
            "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Predicted text from audio is PLIY1Z KAA1L STEH0LAX0 AE1SK HHER1R TUW1 BRIH1NG DHIY1Z THIH1NGZ WIH1TH HHER1R FRAH1M DAX1 STAO1R SIH1KS SPUW1N AX1F FREH1SH SHNOW0UH PIY1Z FAE0IH0V THIH1K SLAE1BZ AX1V BLUW1 CHIY1Z E1N MEH0IH0BIY0 AX1 SNAE1K FAO1 HHER1 BRAH0DHEX0 BAA1B WIY1 AA0LSOW0 NIY1D AX1 SMAA1L PLAE0STIH0K SNEH0IH0K E1ND AX1 BIY1K TOY1 FRAA1G FAO1 DHAX1 KIH1DZ SHIY1 KAX1N SKUW1P DIY1Z THIH1NGZ IH0NTUW0 THRIY1 REH1D BAE1GZ AE1N WIY1 WIH1L GOW0UH MIY1T HHER1 WEH0NZDEH0IH0 AE1T DHAX1 TREH0IH0N STEH0IH0SHAX0N\n",
            "wav2arpaf/russian20.txt\n",
            "tagalog5.wav\n",
            "Predicted text from audio is PLIY1S KAA1L STEH0LAE0 AE1SK HHER1 TUW1 BRIH1NG DHIH1S THIH1NGS WIH1TH HHER1R FRAH1M DHAX1 STAO1R SIH1KS SPUW1NZ AX1F FREH1SH SNOW0UH0 PIY1S FAE0IH0V THIH1K SLAE1BZ AX1V BLUW1 CHIY1S EH1N MEH0IH0BIY0 AX1 SNAE1K FAO1 HHER1 BRAH0DHAEX0R BAO1B WIY1 AA0LSOW0 NIY1D AX1 SMAO1L PLAE0STIH0K SNEH0IH0K AEH1N AX1 BIH1G TOY1 FRAA1G FAO1R DHAX1 KIH1DS SHIY1 KAH1N SKUW1P DHIH1S THIH1NGS IH0NTUW0 THRIY1 REH1D BAE1GS AE1N WIY1 WIH1L GOW0UH0 MIY1T HHER1R WEH0NZDEH0IH0 AE1T DAX1 TREH0IH0N STEH0IH0SHAX0N\n",
            "wav2arpaf/tagalog5.txt\n",
            "mandarin51.wav\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Predicted text from audio is PLIY1S KAO1L STEH0LAE0 AE1SK HHER1 TUW1 BRIH1NG DIY1S SIY1NGS WIH1Z HHER1 FRAH1M DAX1 STAO1 SIH1KS SPUW1NS AO1F FREH1SH SNOW0UH0 BPIY1S FAE1F SIY1K SLAE1PS A1F BLUW1 CHIY1S AE1NDAX0 MEH0IH0BIY0 AX1 SNAE1K FAO1 HHER1 BRAH0DHEX0 BAO1B WIY1 AO0LSOW0 NIY1D AX1 SMAO1L PLAE0SIH0K SNEH0IH0K AE1N AX1 BIY1K TOY1 FRAO1G FAO1R HHER1 KIY1DZ SHIY1 KAE1 SKUW1P ZIY1S SIY1NGS IH0NTUW0 SRIY1 REH1D BAE1KS AE1N WIY1 WIH1L GOW0UH MIY1T HHER1R EH0SDEHIH0 AE1T DAX1 TREH0IH0N STEH0IH0SHAX0N\n",
            "wav2arpaf/mandarin51.txt\n",
            "spanish55.wav\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
            "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Predicted text from audio is PLIY1Z KAO1L STEH0LAX0 AE1SK HHER1 TUW1 BRIH1NG DHIY1Z THIH1NGZ WIH1TH HHER1R FRAX1M DAX1 STAO1R SIY1KS SPUW1NZ AX1V FREH1SH SHNOW0UH0 PIY1Z FAE0IH0FTHIH1K SLAE1BZ AX1V BLUW1 CHIY1Z AE1 MEH0IH0BIY0 AX1 SNAE1K FAO1R HEX1 BRAH0DHAX0 BAA1B WIY1 AO0LSO0 NIY1D AX1 SMAO1L PLAE0STIH0K SNEH0IH0K AE1N EH1 BIY1G TOY1 FRAO1G FAX1 DHAX1 KIH1DZ SHIY1 KAE1N SKUW1P DIY1Z THIH1NGZ IH0NTUW0 THRIY1 REH1D BAE1GZ AE1N WIY1 WIH1L GOW01 MIY1T HHER1 WEH0NZDEH0IH0 AE1T DAX1 TREH0IH0N STEH0IH0SHAX0N\n",
            "wav2arpaf/spanish55.txt\n",
            "romanian10.wav\n",
            "Predicted text from audio is PLIY1S KAO1L STEH0LAE0 AE1SK HHER1 TUW1 BRIH1NG DIH1S THIH1NGS WIH1TH HHER1 FRAH1M DHAX1 STAO1R SIH1KSSPUW1NS AX1F FREH1SH SNOW0UH PIY1S FAE0IH0F THIH1K SLAE1PS AX1FV BLUW1 CHIY1Z AE1N MEH0IH0BIY0 AX1 SNAE1K FER1R HHER1R BRAH0DHAX0 BA1B WIY1 AA0LSOW0 NIY1D AX1 SMAO1L PLAE0STIH0K SNEH0IH0K AE1N AX1 BIY1G TOY1 FRAO1G FAO1R DHAX1 KIH1DS SHIY1 KE1N SKUW1P DHIH1S THIH1NGZ IH0NTUW0 THRIY1 REH1D BAE1GS E1N WIY1 WIH1L GOW0UH0 MIY1T HHE1R WEH0NZDEH0IH0 AE1T DHAX1 TREH0IH0N STEH0IH0SHAX0N\n",
            "wav2arpaf/romanian10.txt\n",
            "spanish31.wav\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Predicted text from audio is PLIY1S KO1L EH0STEH0LAE0 AE1SK HHEH1R TUW1 BRIH1NG DIY1S THIH1NGK WIH1TH HHEH1R FRAO1 DIEH1 STAO1R SIH1KAX0SPUW1NS AO1F FREH1SH H0SNOW0UH0 PIY1S FAE0IH0F THIH1NK XH1 SLAE1P AO1F BLUW1 CHIY1S AE1N MEH0IH0BIY0 AEH0 SNAE1K FAO1R HHEH1S HHEH1R BRAO0DHE0R BAO1 WIY1 AO0LSOW0 NIY1D AEH0 SMAO1L PLAE0STIY0K EH0 SNEH0IH0K AE1N AX1 BIY1G TOY1 FRAO1 FAO1R DHAX1 KIY1TS SHIY1 KEH1N EH0 SKUW1P DIY1S THIH1NGK IH0NTUW0 THRIY1 REH1D BAE1KS AE1N WIY1 WIH1L GOW01H MIY1T HHEH1R WEH0NAX0SDEH0IH0 AE1NDEH10 TREH0IH0N EH1 STEH0IH0SHAX0N\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "wav2arpaf/spanish31.txt\n",
            "english12.wav\n",
            "Predicted text from audio is PLIY1S KAO1L STEH0LAX0 AE1SK HHER1 TUX1 BRIH1NG DHIY1Z SIH1NGZ WIH1TH HHER1 FRAX1M NAX1 STAO1 SIH1KS SPUW1NZ AX1F FREH1SH SHNOW0UH PIY1Z FAE0IH0V THIH1K SLAE1BS AX1V BLUW1 CHIY1Z AEH1N MEH0IH0BIY0 AX1 SNAE1K FAX1 HHER1R BRAH0DHAX0 BAO1B WIY1 AO0LSOW0 NIY1D AX1 SMAO1L PLAE0STIH0K SNEH0IH0K AE1N AX1 BIH1G TOY1 FRAO1G FAX1 DHAX1 KIH1TS SHIY1 KAX1N SKUW1P DHIY1Z SIH1NZ IH0NTAX0 THRIY1 REH1D BAE1GZ AX1N WIH1L GOW0UH0 TUW EH1N WIY1 WIH1L GOW0UH MIY1T HHER1 WEH0NZDEH0IH0 AE1T DHAX1 TREH0IH0N STEH0IH0SHAX0N\n",
            "wav2arpaf/english12.txt\n",
            "arabic53.wav\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Predicted text from audio is PLIY1Z KAO1L STEH0LAE0 AE1SK HHER1R TUW1 BRIH1NG DIY1Z THIH1NGKS WIH1TH HHER1 FRAH1M DAX1 STAO1R SIH1KS SPUW1NZ AO1VWAX FREH1SH SNOW0UH0 PIY1 FAE0IH0V THIH1K SLAE1BZ AO1V BLUW1 CHIY1Z EH1N MEH0IH0BIY0 AX1 SNAE1K FO1R HHER1R BRAH0DHAX0 BAO1B WIY1 AO0LSOW0 NIY1D AX1 SMAO1L PLAE0SIH0K SNEH0IH0K AE1 AE1N AX1 BIY1G TOY1 FRAO1G FAO1R DHAX1 KIH1DS SHIY1 KAE1N SKUW1P DIY1Z THIH1NGKS IH0NTUW0 TRIY1 AX1REH1D BAE1GS AEH1N WIY1 WIH1L GOW1H MIY1T DHAH HHER1R WEH0NZDEH0IH0 AE1T DHAX1 TREH0IH0N STEH0IH0SHAX0N\n",
            "wav2arpaf/arabic53.txt\n",
            "italian12.wav\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Predicted text from audio is PLIY1S KOW1L STEH0LAE0 AE1SK HHER1 TUW1 BRIH1NG DIY1S TIH1NGS WIH1T HHER1R FRAO1M DAX1 STAO1R SIH1KS SPUW1NZ AO1F FREH1SH SNOW1 PIY1S FAE0IH0F TIH1K SLAE1BS AO1F BLUW1 CHIY1S E1N MEH0IH0BIY0 EH1 SNAE1K FAO1R HHER1R BRAH0DHAX0R BOW1B WIY1 AO0LSOW0 NIY1D AX1 SMAO1L PLAE0STIH0K SNEH0IH0K AE1ND AX1 BIY1G TOY1 TOY1 FRAO1G FAO1R DAX1 KIY1DS SHIY1 KAX1N SKUW1P DIY1S TIH1NGS IH0NTUW0 TRIY1 REH1D BAE1GS AEH1ND WIY1 WIH1L GOW1 MIY1T HHER1R WEH0NZDEH0IH0 AE1T DAX1 TREH0IH0N STEH0IH0SHAX0N\n",
            "wav2arpaf/italian12.txt\n",
            "russian14.wav\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Predicted text from audio is PLIY1Z KAO1L STEH0LAX0 AE1SK HHER1R TUW1 BRIH1N DHIY1S TIH1NGS FRAO1M HHER1R VIH1TH HHER1R FRAH1M DHAX1 STAO1R SIH1KS SPUW1NS AO1F FREH1SH SNOW01H SHAX R1 SNOW0UH PIY1Z FAE0IH0F THIH1K SLAE1BZ AO1V BLUW1 CHIY1Z AE1N MEH0IH0BIY0 AX1 SNAE1K FAO1R HHER1 BRAH0DHAX0 BAO1B VIY1 AO0LSOW0 NIY1D AX1 SMAO1L PLAE0STIH0K SNEH0IH0K AE1ND AX1 BIY1G TOY1 FRAO1G FAO1R DHAX1 KIY1DZ SHIY1 KAE1N SKUW1P DHIH1T THIH1NGS IH0NTUW0 THRIY1 REH1D BAE1GS AE1ND VIY1 VIH1L GOW01H MIY1T HHER1 VEH0NZDEH0IH0 AE1T DAX1 TREH0IH0N STEH0IH0SHAX0N\n",
            "wav2arpaf/russian14.txt\n",
            "english24.wav\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
            "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Predicted text from audio is PLIY1Z KAO1L STEH0LAE0 AE1SK HER1 TUW1 BRIH1NG DIY1Z THIH1NGZ WIH1TH ER1 FRAH1M NHAX1 STAO1R SIH1KS SPU1NZ AX1V FREH1SH SNOW1H PIY1Z FAE0IH0V FIH1K SLAE1BZ AX1V BLUW1 CHIY1Z AX1N MEH0IH0BIY0 AX1 SNAE1K FAX1R HER1 BRAH0DHAX0 BOW1B WIY1 AO0LSOW0 NIY1D AX1 SMAO1L PLAE0STIH0K SNEH0IH0K AX1N X1BIH1G TOY1 FRAOW1G FAX1 DHAX1 KIH1DS SHIY1 KAX1N SKU1P DHIY1Z SHIH1NGZ IH0NTUW0 THRIY1 REH1D BAE1G AE1N WIH1L GOW1H MIY1T HHER1 AX1N WEH0DZNZEHIH0 A1T DHAX1 TREH0IH0N STEH0IH0SHAX0N\n",
            "wav2arpaf/english24.txt\n",
            "tatar1.wav\n",
            "Predicted text from audio is PLIY1S KAOW1L STEH0LAE0 AE1S HHER1R TUW1 BRIH1NG DHIY1S THIH1NGZ WIH1FH HHER1R FRAH1M DAX1 STAO1R SIH1KS SPUW1NZ AO1F FREH1SH SNOW1 PIY1S FAE0IH0F THIH1K SLAE1BZ AO1V BLUW1 CHIY1Z AEH1N MEH0IH0BIY0 AX1 SNAE1K FAO1R HHER1 BRAH0DHAX0 BAO1B WIY1 AO0LSOW0 NIY1D AX1 SMAO1L PLAE0STIH0K SNEH0IH0K AE1ND AX1 BIH1K TOY1 FRAO1G FAO1R DHAX1 KIH1DS SHIY1 KAE1N SKUW1P DHIY1S THIH1NGZ IH0NTUW0 THRIY1 REH1D BAE1GZ E1N WIY1 WIH1L GOW1H MIY1T HHER1R WEH0NZDEH0IH0 AE1T DAX1 TREH0IH0N STEH0IH0SHAX0N\n",
            "wav2arpaf/tatar1.txt\n",
            "german3.wav\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
            "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Predicted text from audio is PLIY1S KAA1L STEH0LAX0 AE1SK HHER1R TUW1 BRIH1NG DIY1S THIH1NGS WIH1TH HHER1R FRAH1M DHAX1 STAO1R SIH1KS SPUW1NS AX1F FREH1SH SNOW01H PIY1S FAE0IH0F THIH1KS AH1M SLAE1PS AX1F BLUW1 CHIY1Z AH1N MEH0IH0BIY0 AX1 SNAE1K FAO1 HHER1R BRAH0DHAX0 BAA1B VIY1 AA0LZOW0 NIY1D AX1 SMAO1L PLAE0STIH0K SNEH0IH0K EH1N AX1 BIH1K TOY1 FRAO1G FAO1R DHAX1 KIH1DS VIY1 KAE1N SKUW1P DHIY1S THIH1NGS IH0NTUW0 THRIY1 REH1D BAE1KS AH1N VIY1 VIH1L GOW01H MIY1T HHER1 WEH0NZDEH0IH0 AE1T DAX1 TREH0IH0N STEH0IH0SHAX0N\n",
            "wav2arpaf/german3.txt\n",
            "serbian8.wav\n",
            "Predicted text from audio is PLIY1S KAO1L STEH0LAE0 AE1SK HHER1R TUW1 BRIH1NG DIY1S TIH1NGKS VIH1TH HHER1R FRAH1M DAX1 STAO1R SIH1KS SPUW1NS AO1F FREH1SH SNOW0UH PIY1S FAE0IH0V TIH1K SLAE1BS AO1V BLUW1 CHIY1S AE1N MEH0IH0BIY0 AX1 SNEH0IH0K FAO1R HHER1 BRAH0DHAX0 BA1B VIY1 AO0LSOW0 NIY1D AX1 SMAO1L PLAE0STIH0K SNEH0IH0K AH1ND AX1 BAIH1GK BIH1K TOY1 FRAO1G FAO1R DAX1 KIY1TS SHIY1 KEH1N SKUW1P DIY1S TIY1NGKS IH0NTUW0 TRIY1 REH1D BAE1KS AE1N VIY1 VIH1L GOW01H MIY1T F HHER1 VEH0NZDEH0IH0 AE1T DAX1 DAX1 TREH0IH0N STEH0IH0SHAX0N\n",
            "wav2arpaf/serbian8.txt\n",
            "kurdish6.wav\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Predicted text from audio is PLIY1S KAO1L STEH0LAE0 AE1SK HHEH1R TUW1 BRIH1NG DHIY1S THIH1NGKAX0S WIH1TH HHEH1R FRAH1M DHAX1 STAO1R SIH1KS SPUW1NS AO1F FREH1SH SNOW0UH PIY1S FAE0IH0F THIH1K SLAE10VAX0S AO1F BLUW1 CHIY1ZAX0 AE1NDT MEH0IH0BIY0 EH1 SNAE1K FAO1R HHEH1R BRAH0DHAER0R BAA1 WIY1 AO0LSOW0 NIY1D EH1 SMAO1L PLAE0STIY0K SNEH0IH1K AE1ND EH1 BIH1K TOY1 FRAO1G FAO1R DHAX1 KIY1TS SHIY1 KAE1N SKUW1P DHIY1S TFHIH1NGAX0Z IH0NTUW0 THRIY1 REIHAE1T BAE1GS AE1NT WIY1 WIH1L GOW1 MIY1T HHEH1RZ WEH0NAX0ZDEH0IH0 AE1T DHAX1 TREH0IH0N STEH0IH0SHAX0N\n",
            "wav2arpaf/kurdish6.txt\n",
            "russian3.wav\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Predicted text from audio is PLIY1Z KAO1L STEH0LAE0 AE1SK HHER1R TUW1 BRIH1N DHIY1Z THIH1NGZ WIH1TH HHAR1 FRAH1M DHAX1 STAO1R SIH1KS SPUW1NZ AX1F FREH1SH SNOW0UH PIY1Z FAE0IH0V THIH1K SLAE1BZ AO1V BLUW1 CHIY1Z E1N MEH0IH0BIY0 AX1 SNAE1K FAO1R HHEX1 BRAH0DHAX0 BAA1B WIY1 AO0LSOW0 NIY1D AX1 SMAO1L PLAE0STIH0K SNEH0IH0K AE1ND AX1 BIH1G TOY1 FRAO1G FAO1 DHAX1 KIH1DZ SHIY1 KAX1N SKUW1P DHIH1S THIH1NGZ IH0NTUW0 THRIY1 REH1D BAE1GZ AE1ND WIY1 WIH1L GOW01H MIY1T HHEAR1 WEH0NZDEH0IH0 AE1T DHAX1 TREH0IH0N STEH0IH0SHAX0N\n",
            "wav2arpaf/russian3.txt\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!zip -r /content/wav2arpa_finetuned.zip /content/wav2arpaf"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hl6KHMiUdbuY",
        "outputId": "bb34e74c-3edb-4f64-a828-2548a4137315"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  adding: content/wav2arpaf/ (stored 0%)\n",
            "  adding: content/wav2arpaf/bulgarian1.txt (deflated 46%)\n",
            "  adding: content/wav2arpaf/mankanya1.txt (deflated 48%)\n",
            "  adding: content/wav2arpaf/spanish3.txt (deflated 48%)\n",
            "  adding: content/wav2arpaf/dari6.txt (deflated 46%)\n",
            "  adding: content/wav2arpaf/danish2.txt (deflated 46%)\n",
            "  adding: content/wav2arpaf/arabic17.txt (deflated 46%)\n",
            "  adding: content/wav2arpaf/arabic53.txt (deflated 46%)\n",
            "  adding: content/wav2arpaf/tatar1.txt (deflated 46%)\n",
            "  adding: content/wav2arpaf/greek2.txt (deflated 48%)\n",
            "  adding: content/wav2arpaf/korean20.txt (deflated 46%)\n",
            "  adding: content/wav2arpaf/tagalog5.txt (deflated 46%)\n",
            "  adding: content/wav2arpaf/italian22.txt (deflated 46%)\n",
            "  adding: content/wav2arpaf/mandarin1.txt (deflated 48%)\n",
            "  adding: content/wav2arpaf/hebrew3.txt (deflated 47%)\n",
            "  adding: content/wav2arpaf/english38.txt (deflated 46%)\n",
            "  adding: content/wav2arpaf/hindi3.txt (deflated 47%)\n",
            "  adding: content/wav2arpaf/tamil3.txt (deflated 46%)\n",
            "  adding: content/wav2arpaf/german15.txt (deflated 46%)\n",
            "  adding: content/wav2arpaf/spanish19.txt (deflated 49%)\n",
            "  adding: content/wav2arpaf/portuguese25.txt (deflated 46%)\n",
            "  adding: content/wav2arpaf/dutch41.txt (deflated 46%)\n",
            "  adding: content/wav2arpaf/serbian8.txt (deflated 47%)\n",
            "  adding: content/wav2arpaf/spanish55.txt (deflated 46%)\n",
            "  adding: content/wav2arpaf/spanish31.txt (deflated 49%)\n",
            "  adding: content/wav2arpaf/ukranian8.txt (deflated 46%)\n",
            "  adding: content/wav2arpaf/russian3.txt (deflated 45%)\n",
            "  adding: content/wav2arpaf/bosnian3.txt (deflated 46%)\n",
            "  adding: content/wav2arpaf/luba-kasai1.txt (deflated 46%)\n",
            "  adding: content/wav2arpaf/georgian5.txt (deflated 48%)\n",
            "  adding: content/wav2arpaf/russian20.txt (deflated 46%)\n",
            "  adding: content/wav2arpaf/arabic24.txt (deflated 49%)\n",
            "  adding: content/wav2arpaf/english24.txt (deflated 45%)\n",
            "  adding: content/wav2arpaf/kirghiz1.txt (deflated 47%)\n",
            "  adding: content/wav2arpaf/spanish58.txt (deflated 46%)\n",
            "  adding: content/wav2arpaf/portuguese4.txt (deflated 48%)\n",
            "  adding: content/wav2arpaf/portuguese3.txt (deflated 46%)\n",
            "  adding: content/wav2arpaf/italian12.txt (deflated 47%)\n",
            "  adding: content/wav2arpaf/cantonese1.txt (deflated 48%)\n",
            "  adding: content/wav2arpaf/mandarin51.txt (deflated 46%)\n",
            "  adding: content/wav2arpaf/german21.txt (deflated 47%)\n",
            "  adding: content/wav2arpaf/french6.txt (deflated 46%)\n",
            "  adding: content/wav2arpaf/macedonian1.txt (deflated 46%)\n",
            "  adding: content/wav2arpaf/romanian4.txt (deflated 46%)\n",
            "  adding: content/wav2arpaf/gujarati5.txt (deflated 46%)\n",
            "  adding: content/wav2arpaf/thai2.txt (deflated 47%)\n",
            "  adding: content/wav2arpaf/somali3.txt (deflated 47%)\n",
            "  adding: content/wav2arpaf/french34.txt (deflated 46%)\n",
            "  adding: content/wav2arpaf/english44.txt (deflated 45%)\n",
            "  adding: content/wav2arpaf/mongolian3.txt (deflated 47%)\n",
            "  adding: content/wav2arpaf/croatian6.txt (deflated 48%)\n",
            "  adding: content/wav2arpaf/hausa7.txt (deflated 46%)\n",
            "  adding: content/wav2arpaf/swedish2.txt (deflated 46%)\n",
            "  adding: content/wav2arpaf/romanian10.txt (deflated 46%)\n",
            "  adding: content/wav2arpaf/turkish9.txt (deflated 46%)\n",
            "  adding: content/wav2arpaf/spanish17.txt (deflated 47%)\n",
            "  adding: content/wav2arpaf/norwegian5.txt (deflated 46%)\n",
            "  adding: content/wav2arpaf/japanese12.txt (deflated 46%)\n",
            "  adding: content/wav2arpaf/arabic1.txt (deflated 46%)\n",
            "  adding: content/wav2arpaf/russian14.txt (deflated 47%)\n",
            "  adding: content/wav2arpaf/tibetan2.txt (deflated 48%)\n",
            "  adding: content/wav2arpaf/nepali9.txt (deflated 46%)\n",
            "  adding: content/wav2arpaf/serbian10.txt (deflated 45%)\n",
            "  adding: content/wav2arpaf/kurdish6.txt (deflated 47%)\n",
            "  adding: content/wav2arpaf/farsi5.txt (deflated 49%)\n",
            "  adding: content/wav2arpaf/english12.txt (deflated 46%)\n",
            "  adding: content/wav2arpaf/farsi10.txt (deflated 48%)\n",
            "  adding: content/wav2arpaf/english2.txt (deflated 46%)\n",
            "  adding: content/wav2arpaf/vietnamese9.txt (deflated 45%)\n",
            "  adding: content/wav2arpaf/uyghur4.txt (deflated 46%)\n",
            "  adding: content/wav2arpaf/portuguese10.txt (deflated 45%)\n",
            "  adding: content/wav2arpaf/bengali2.txt (deflated 45%)\n",
            "  adding: content/wav2arpaf/slovak4.txt (deflated 47%)\n",
            "  adding: content/wav2arpaf/hungarian5.txt (deflated 47%)\n",
            "  adding: content/wav2arpaf/swedish4.txt (deflated 46%)\n",
            "  adding: content/wav2arpaf/wolof3.txt (deflated 46%)\n",
            "  adding: content/wav2arpaf/english139.txt (deflated 47%)\n",
            "  adding: content/wav2arpaf/konkani2.txt (deflated 45%)\n",
            "  adding: content/wav2arpaf/rwanda1.txt (deflated 47%)\n",
            "  adding: content/wav2arpaf/serbian17.txt (deflated 46%)\n",
            "  adding: content/wav2arpaf/english55.txt (deflated 48%)\n",
            "  adding: content/wav2arpaf/english112.txt (deflated 47%)\n",
            "  adding: content/wav2arpaf/arabic8.txt (deflated 46%)\n",
            "  adding: content/wav2arpaf/shilluk1.txt (deflated 46%)\n",
            "  adding: content/wav2arpaf/tokpisin1.txt (deflated 45%)\n",
            "  adding: content/wav2arpaf/finnish1.txt (deflated 47%)\n",
            "  adding: content/wav2arpaf/turkish20.txt (deflated 47%)\n",
            "  adding: content/wav2arpaf/turkish15.txt (deflated 45%)\n",
            "  adding: content/wav2arpaf/yoruba3.txt (deflated 46%)\n",
            "  adding: content/wav2arpaf/krio2.txt (deflated 47%)\n",
            "  adding: content/wav2arpaf/spanish47.txt (deflated 48%)\n",
            "  adding: content/wav2arpaf/spanish4.txt (deflated 48%)\n",
            "  adding: content/wav2arpaf/tagalog3.txt (deflated 47%)\n",
            "  adding: content/wav2arpaf/mandarin24.txt (deflated 46%)\n",
            "  adding: content/wav2arpaf/german3.txt (deflated 47%)\n",
            "  adding: content/wav2arpaf/yakut1.txt (deflated 46%)\n",
            "  adding: content/wav2arpaf/arabic2.txt (deflated 45%)\n",
            "  adding: content/wav2arpaf/russian25.txt (deflated 49%)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "4P9-Kqvbn3sL"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}